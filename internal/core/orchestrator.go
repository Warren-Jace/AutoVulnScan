// Package core åŒ…å«äº† AutoVulnScan åº”ç”¨ç¨‹åºçš„æ ¸å¿ƒç¼–æ’å™¨ã€‚
package core

import (
	"bytes"
	"context"
	"crypto/md5"
	"fmt"
	"io"
	"net/http"
	"net/url"
	"regexp"
	"sort"
	"strconv"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"autovulnscan/internal/ai"
	"autovulnscan/internal/browser"
	"autovulnscan/internal/config"
	"autovulnscan/internal/crawler"
	"autovulnscan/internal/dedup"
	"autovulnscan/internal/models"
	"autovulnscan/internal/output"
	"autovulnscan/internal/requester"
	"autovulnscan/internal/vulnscan"
	_ "autovulnscan/internal/vulnscan/plugins" // åŒ¿åå¯¼å…¥ä»¥æ‰§è¡Œæ’ä»¶çš„init()å‡½æ•°è¿›è¡Œæ³¨å†Œ

	"github.com/rs/zerolog/log"
	"golang.org/x/net/html"
)

// PageStructure é¡µé¢ç»“æ„ä¿¡æ¯
type PageStructure struct {
	DOMHash     string            // DOMç»“æ„å“ˆå¸Œ
	TextHash    string            // æ–‡æœ¬å†…å®¹å“ˆå¸Œ
	FormFields  map[string]string // è¡¨å•å­—æ®µæ˜ å°„
	InputCount  int               // è¾“å…¥å­—æ®µæ•°é‡
	LinkCount   int               // é“¾æ¥æ•°é‡
	ScriptCount int               // è„šæœ¬æ•°é‡
	Title       string            // é¡µé¢æ ‡é¢˜
}

// URLPattern URLæ¨¡å¼
type URLPattern struct {
	BaseURL    string   // åŸºç¡€URL
	ParamNames []string // å‚æ•°ååˆ—è¡¨
	Pattern    string   // URLæ¨¡å¼
}

// SimilarityConfig ç›¸ä¼¼åº¦é…ç½®
type SimilarityConfig struct {
	DOMThreshold     float64 // DOMç»“æ„ç›¸ä¼¼åº¦é˜ˆå€¼
	ContentThreshold float64 // å†…å®¹ç›¸ä¼¼åº¦é˜ˆå€¼
	FormThreshold    float64 // è¡¨å•ç›¸ä¼¼åº¦é˜ˆå€¼
	URLThreshold     float64 // URLæ¨¡å¼ç›¸ä¼¼åº¦é˜ˆå€¼
	AutoAdjust       bool    // æ˜¯å¦è‡ªåŠ¨è°ƒæ•´é˜ˆå€¼
}

// Orchestrator è´Ÿè´£åè°ƒçˆ¬è™«ã€æ‰«æå’ŒæŠ¥å‘Šçš„ä¸»æµç¨‹æ§åˆ¶å™¨ã€‚
type Orchestrator struct {
	config       *config.Settings
	targetURL    string
	crawler      *crawler.Crawler
	scanEngine   *vulnscan.Engine // ä½¿ç”¨æ‰«æå¼•æ“
	deduplicator *dedup.Deduplicator
	aiAnalyzer   *ai.AIAnalyzer
	httpClient   *requester.HTTPClient
	ctx          context.Context
	cancel       context.CancelFunc

	stats struct {
		urlsProcessed        int64
		requestsScanned      int64
		vulnerabilitiesFound int64
		duplicatesSkipped    int64
		similarPagesSkipped  int64
	}

	retryConfig struct {
		maxRetries int
		retryDelay time.Duration
	}

	similarityConfig SimilarityConfig
	pageStructures   sync.Map
	urlPatterns      sync.Map
	formStructures   sync.Map
	requestDedup     sync.Map
	domainStats      map[string]*DomainStatistics
	domainStatsMutex sync.RWMutex
}

// DomainStatistics åŸŸåç»Ÿè®¡ä¿¡æ¯ï¼Œç”¨äºåŠ¨æ€è°ƒæ•´é˜ˆå€¼
type DomainStatistics struct {
	TotalPages        int       // æ€»é¡µé¢æ•°
	UniqueForms       int       // å”¯ä¸€è¡¨å•æ•°
	AverageSimilarity float64   // å¹³å‡ç›¸ä¼¼åº¦
	LastAdjustment    time.Time // æœ€åè°ƒæ•´æ—¶é—´
}

// FormStructure è¡¨å•ç»“æ„
type FormStructure struct {
	Fields []string // å­—æ®µååˆ—è¡¨
	Types  []string // å­—æ®µç±»å‹åˆ—è¡¨
	Action string   // è¡¨å•action
	Method string   // è¡¨å•method
	Hash   string   // ç»“æ„å“ˆå¸Œ
}

// NewOrchestrator åˆ›å»ºå¹¶åˆå§‹åŒ–ä¸€ä¸ªOrchestratorå®ä¾‹ã€‚
// è¿™ä¸ªå‡½æ•°è´Ÿè´£ç»„è£…æ‰€æœ‰å¿…è¦çš„ç»„ä»¶ï¼Œå¦‚HTTPå®¢æˆ·ç«¯ã€çˆ¬è™«ã€æ‰«æå¼•æ“ç­‰ã€‚
func NewOrchestrator(cfg *config.Settings, targetURL string) (*Orchestrator, error) {
	ctx, cancel := context.WithCancel(context.Background())

	httpClient := requester.NewHTTPClient(cfg.Spider.Timeout, cfg.Headers)

	cr, err := crawler.NewCrawler(targetURL, cfg, httpClient)
	if err != nil {
		cancel()
		return nil, fmt.Errorf("åˆå§‹åŒ–çˆ¬è™«å¤±è´¥: %w", err)
	}

	// åˆå§‹åŒ–æµè§ˆå™¨æœåŠ¡
	var browserService *browser.BrowserService
	if cfg.Spider.DynamicCrawler.Enabled {
		browserService, err = browser.NewBrowserService(browser.Config{
			Headless:  cfg.Spider.DynamicCrawler.Headless,
			Proxy:     cfg.Proxy,
			UserAgent: cfg.Headers["User-Agent"],
		})
		if err != nil {
			log.Warn().Err(err).Msg("åˆå§‹åŒ–æµè§ˆå™¨æœåŠ¡å¤±è´¥ï¼Œéƒ¨åˆ†åŠŸèƒ½ï¼ˆå¦‚XSS DOMéªŒè¯ï¼‰å°†å—é™")
			// éè‡´å‘½é”™è¯¯ï¼Œå…è®¸ç»§ç»­
		}
	}

	scanEngine, err := vulnscan.NewEngine(httpClient, browserService)
	if err != nil {
		cancel()
		return nil, fmt.Errorf("åˆå§‹åŒ–æ‰«æå¼•æ“å¤±è´¥: %w", err)
	}

	var aiAnalyzer *ai.AIAnalyzer
	if cfg.AIModule.Enabled {
		aiAnalyzer, err = ai.NewAIAnalyzer(cfg.AIModule.APIKey, cfg.AIModule.Model, "")
		if err != nil {
			log.Warn().Err(err).Msg("åˆå§‹åŒ–AIåˆ†æå™¨å¤±è´¥ï¼ŒAIåŠŸèƒ½å°†è¢«ç¦ç”¨")
		}
	}

	o := &Orchestrator{
		config:       cfg,
		targetURL:    targetURL,
		crawler:      cr,
		scanEngine:   scanEngine,
		deduplicator: dedup.NewDeduplicator(0.95), // ä½¿ç”¨é»˜è®¤é˜ˆå€¼
		aiAnalyzer:   aiAnalyzer,
		httpClient:   httpClient,
		ctx:          ctx,
		cancel:       cancel,
		domainStats:  make(map[string]*DomainStatistics),
	}

	// åˆå§‹åŒ–é‡è¯•é…ç½®
	o.retryConfig.maxRetries = 3
	o.retryConfig.retryDelay = 2 * time.Second

	// åˆå§‹åŒ–ç›¸ä¼¼åº¦é…ç½®
	o.initSimilarityConfig()

	return o, nil
}

// isInScope æ£€æŸ¥ç»™å®šçš„URLæ˜¯å¦åœ¨æ‰«æèŒƒå›´å†…ã€‚
// å®ƒä¼šæ ¹æ®é…ç½®çš„åŸŸåèŒƒå›´å’Œé»‘åå•è¿›è¡Œåˆ¤æ–­ã€‚
func (o *Orchestrator) isInScope(link string) bool {
	parsedURL, err := url.Parse(link)
	if err != nil {
		log.Debug().Str("url", link).Err(err).Msg("æ— æ³•è§£æURLï¼Œå·²è·³è¿‡")
		return false
	}

	// æ£€æŸ¥URLæ˜¯å¦åœ¨é»‘åå•ä¸­
	for _, blacklistedPattern := range o.config.Blacklist {
		if matched, _ := regexp.MatchString(blacklistedPattern, link); matched {
			return false
		}
	}

	// æ£€æŸ¥URLåŸŸåæ˜¯å¦åœ¨èŒƒå›´å†…
	for _, scopeDomain := range o.config.Scope {
		if strings.HasSuffix(parsedURL.Host, scopeDomain) {
			return true
		}
	}

	return false
}

// initSimilarityConfig åˆå§‹åŒ–ç›¸ä¼¼åº¦é…ç½®
func (o *Orchestrator) initSimilarityConfig() {
	o.similarityConfig = SimilarityConfig{
		DOMThreshold:     0.85, // DOMç»“æ„ç›¸ä¼¼åº¦é˜ˆå€¼85%
		ContentThreshold: 0.80, // å†…å®¹ç›¸ä¼¼åº¦é˜ˆå€¼80%
		FormThreshold:    0.90, // è¡¨å•ç›¸ä¼¼åº¦é˜ˆå€¼90%
		URLThreshold:     0.75, // URLæ¨¡å¼ç›¸ä¼¼åº¦é˜ˆå€¼75%
		AutoAdjust:       true, // å¯ç”¨è‡ªåŠ¨è°ƒæ•´
	}
}

// Start å¯åŠ¨ç¼–æ’å™¨çš„æ€»æ‰§è¡Œæµç¨‹ã€‚
func (o *Orchestrator) Start(reporter *output.Reporter) {
	log.Info().Str("target", o.targetURL).Msg("âœ… ç¼–æ’å™¨å¯åŠ¨ (Orchestrator started)")
	defer func() {
		o.printFinalStats()
		log.Info().Str("target", o.targetURL).Msg("âœ… ç¼–æ’å™¨æ‰§è¡Œå®Œæ¯• (Orchestrator finished)")
		o.cancel()
	}()

	// å¯åŠ¨ç»Ÿè®¡ä¿¡æ¯å®šæœŸè¾“å‡º
	statsTicker := time.NewTicker(30 * time.Second)
	defer statsTicker.Stop()
	go o.printStats(statsTicker.C)

	// å¯åŠ¨é˜ˆå€¼è‡ªåŠ¨è°ƒæ•´
	if o.similarityConfig.AutoAdjust {
		adjustTicker := time.NewTicker(5 * time.Minute)
		defer adjustTicker.Stop()
		go o.autoAdjustThresholds(adjustTicker.C)
	}

	var wg sync.WaitGroup
	taskQueue := make(chan models.Task, o.config.Spider.Concurrency*4)

	// å¯åŠ¨å·¥ä½œåç¨‹æ± 
	for i := 0; i < o.config.Spider.Concurrency; i++ {
		go o.worker(i, taskQueue, &wg, reporter)
	}

	// å°†åˆå§‹ç›®æ ‡URLä½œä¸ºç¬¬ä¸€ä¸ªä»»åŠ¡æ·»åŠ åˆ°é˜Ÿåˆ—ä¸­
	log.Info().Str("url", o.targetURL).Msg("å°†åˆå§‹ç›®æ ‡URLæ·»åŠ åˆ°ä»»åŠ¡é˜Ÿåˆ—")
	wg.Add(1)
	taskQueue <- models.Task{URL: o.targetURL, Depth: 0}

	// ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
	wg.Wait()
	close(taskQueue)

	log.Info().Msg("âœ… æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæ¯• (All tasks processed)")
}

// printStats å®šæœŸè¾“å‡ºç»Ÿè®¡ä¿¡æ¯
func (o *Orchestrator) printStats(ticker <-chan time.Time) {
	for range ticker {
		urls := atomic.LoadInt64(&o.stats.urlsProcessed)
		requests := atomic.LoadInt64(&o.stats.requestsScanned)
		vulns := atomic.LoadInt64(&o.stats.vulnerabilitiesFound)
		dups := atomic.LoadInt64(&o.stats.duplicatesSkipped)
		similar := atomic.LoadInt64(&o.stats.similarPagesSkipped)

		log.Info().Msgf("======== ğŸ“ˆ PROGRESS UPDATE ğŸ“ˆ ========\n"+
			"| URLs Processed: %-5d |\n"+
			"| Requests Scanned: %-5d |\n"+
			"| Vulns Found: %-5d |\n"+
			"| Duplicates Skipped: %-5d |\n"+
			"| Similar Pages Skipped: %-5d |\n"+
			"======================================",
			urls, requests, vulns, dups, similar)
	}
}

// printFinalStats è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
func (o *Orchestrator) printFinalStats() {
	urls := atomic.LoadInt64(&o.stats.urlsProcessed)
	requests := atomic.LoadInt64(&o.stats.requestsScanned)
	vulns := atomic.LoadInt64(&o.stats.vulnerabilitiesFound)
	dups := atomic.LoadInt64(&o.stats.duplicatesSkipped)
	similar := atomic.LoadInt64(&o.stats.similarPagesSkipped)

	log.Info().Msgf("============== ğŸ“Š FINAL STATISTICS ğŸ“Š ==============\n"+
		"| Total URLs Processed: %-5d |\n"+
		"| Total Requests Scanned: %-5d |\n"+
		"| Total Vulns Found: %-5d |\n"+
		"| Total Duplicates Skipped: %-5d |\n"+
		"| Total Similar Pages Skipped: %-5d |\n"+
		"===================================================",
		urls, requests, vulns, dups, similar)

	// è¾“å‡ºåŸŸåç»Ÿè®¡
	o.domainStatsMutex.RLock()
	for domain, stats := range o.domainStats {
		log.Info().
			Str("domain", domain).
			Int("total_pages", stats.TotalPages).
			Int("unique_forms", stats.UniqueForms).
			Float64("avg_similarity", stats.AverageSimilarity).
			Msg("ğŸ“ˆ åŸŸåç»Ÿè®¡ (Domain statistics)")
	}
	o.domainStatsMutex.RUnlock()
}

// autoAdjustThresholds è‡ªåŠ¨è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼
func (o *Orchestrator) autoAdjustThresholds(ticker <-chan time.Time) {
	for range ticker {
		o.domainStatsMutex.RLock()
		for domain, stats := range o.domainStats {
			if time.Since(stats.LastAdjustment) < time.Minute*10 {
				continue
			}

			// æ ¹æ®å¹³å‡ç›¸ä¼¼åº¦è°ƒæ•´é˜ˆå€¼
			if stats.AverageSimilarity > 0.9 {
				// é¡µé¢ç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œé™ä½é˜ˆå€¼ä»¥å‡å°‘é‡å¤çˆ¬å–
				o.similarityConfig.DOMThreshold = 0.90
				o.similarityConfig.ContentThreshold = 0.85
			} else if stats.AverageSimilarity < 0.5 {
				// é¡µé¢å·®å¼‚è¾ƒå¤§ï¼Œæé«˜é˜ˆå€¼ä»¥çˆ¬å–æ›´å¤šé¡µé¢
				o.similarityConfig.DOMThreshold = 0.75
				o.similarityConfig.ContentThreshold = 0.70
			}

			stats.LastAdjustment = time.Now()
			log.Debug().
				Str("domain", domain).
				Float64("dom_threshold", o.similarityConfig.DOMThreshold).
				Float64("content_threshold", o.similarityConfig.ContentThreshold).
				Msg("Adjusted similarity thresholds")
		}
		o.domainStatsMutex.RUnlock()
	}
}

// worker å·¥ä½œåç¨‹ï¼Œä¸æ–­ä»ä»»åŠ¡é˜Ÿåˆ—ä¸­å–ä»»åŠ¡å¤„ç†
func (o *Orchestrator) worker(id int, taskQueue chan models.Task, wg *sync.WaitGroup, reporter *output.Reporter) {
	log.Debug().Int("worker_id", id).Msg("ğŸ‘· å·¥ä½œåç¨‹å¯åŠ¨ (Worker started)")
	defer log.Debug().Int("worker_id", id).Msg("ğŸ‘· å·¥ä½œåç¨‹å®Œæˆ (Worker finished)")

	for task := range taskQueue {
		select {
		case <-o.ctx.Done():
			log.Debug().Int("worker_id", id).Msg(" à¦•à¦¾à¦œ Workerå–æ¶ˆ (Worker cancelled)")
			wg.Done()
			return
		default:
		}

		if task.Request != nil {
			// --- å¤„ç†æ‰«æä»»åŠ¡ ---
			log.Debug().
				Int("worker_id", id).
				Str("method", task.Request.Method).
				Str("url", task.Request.URL.String()).
				Msg("âš¡ï¸ æ‰§è¡Œæ‰«æä»»åŠ¡ (Executing scan task)")

			// æ‰§è¡ŒèŒƒå›´æ£€æŸ¥
			if !o.isInScope(task.Request.URL.String()) {
				log.Debug().
					Int("worker_id", id).
					Str("url", task.Request.URL.String()).
					Str("reason", "out_of_scope").
					Msg("â­ï¸ è·³è¿‡æ‰«æä»»åŠ¡ (Skipping scan task)")
				reporter.LogUnscopedURL(task.Request.URL.String())
				wg.Done()
				continue
			}

			requestKey := o.generateRequestKey(task.Request)
			if _, exists := o.requestDedup.LoadOrStore(requestKey, true); exists {
				log.Debug().
					Int("worker_id", id).
					Str("url", task.Request.URL.String()).
					Str("reason", "duplicate_request").
					Msg("â­ï¸ è·³è¿‡æ‰«æä»»åŠ¡ (Skipping scan task)")
				wg.Done()
				continue
			}

			reporter.LogParamURL(task.Request)
			o.scanRequestWithRetry(o.ctx, task.Request, reporter)
			atomic.AddInt64(&o.stats.requestsScanned, 1)

		} else {
			// --- å¤„ç†çˆ¬å–ä»»åŠ¡ ---
			log.Debug().
				Int("worker_id", id).
				Str("url", task.URL).
				Int("depth", task.Depth).
				Msg("ğŸ•¸ï¸ æ‰§è¡Œçˆ¬å–ä»»åŠ¡ (Executing crawl task)")
			o.handleCrawlTask(task, taskQueue, wg, reporter)
		}

		wg.Done()
	}
}

// generateRequestKey ç”Ÿæˆè¯·æ±‚çš„å”¯ä¸€æ ‡è¯†ç¬¦ç”¨äºå»é‡
func (o *Orchestrator) generateRequestKey(req *models.Request) string {
	var keyBuilder strings.Builder
	keyBuilder.WriteString(req.Method)
	keyBuilder.WriteString(":")
	keyBuilder.WriteString(req.URL.String())

	if len(req.Params) > 0 {
		keyBuilder.WriteString("?")
		for i, param := range req.Params {
			if i > 0 {
				keyBuilder.WriteString("&")
			}
			keyBuilder.WriteString(param.Name)
		}
	}

	return keyBuilder.String()
}

// handleCrawlTask å¤„ç†çˆ¬å–ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ·±åº¦æ£€æŸ¥ã€ç›¸ä¼¼åº¦åˆ†æã€é“¾æ¥å’Œè¯·æ±‚å‘ç°
func (o *Orchestrator) handleCrawlTask(task models.Task, taskQueue chan models.Task, wg *sync.WaitGroup, reporter *output.Reporter) {
	// æ³¨æ„ï¼šhandleCrawlTaskä¸å†éœ€è¦è°ƒç”¨wg.Done()ï¼Œå› ä¸ºå®ƒåœ¨workerä¸­è¢«è°ƒç”¨

	// 0. èŒƒå›´æ£€æŸ¥
	if !o.isInScope(task.URL) {
		log.Debug().Str("url", task.URL).Str("reason", "out_of_scope").Msg("â­ï¸ è·³è¿‡çˆ¬å– (Skipping crawl)")
		reporter.LogUnscopedURL(task.URL)
		return
	}

	if task.Depth >= o.config.Spider.MaxDepth {
		log.Debug().Str("url", task.URL).Int("depth", task.Depth).Str("reason", "max_depth_reached").Msg("â­ï¸ è·³è¿‡çˆ¬å– (Skipping crawl)")
		return
	}

	// 1. URLæ¨¡å¼æ£€æŸ¥
	if o.isURLPatternDuplicate(task.URL) {
		log.Debug().Str("url", task.URL).Str("reason", "duplicate_pattern").Msg("â­ï¸ è·³è¿‡çˆ¬å– (Skipping crawl)")
		atomic.AddInt64(&o.stats.similarPagesSkipped, 1)
		return
	}

	// 2. è·å–é¡µé¢å†…å®¹
	log.Debug().Str("url", task.URL).Msg("â¬‡ï¸ æ­£åœ¨è·å–é¡µé¢ (Fetching page)")
	bodyBytes, err := o.fetchURLWithRetry(task.URL)
	if err != nil {
		log.Error().Err(err).Str("url", task.URL).Msg("âŒ è·å–URLå¤±è´¥ (Failed to fetch URL)")
		return
	}
	log.Debug().Str("url", task.URL).Int("size", len(bodyBytes)).Msg("âœ… é¡µé¢è·å–æˆåŠŸ (Page fetched successfully)")

	// 3. åˆ†æé¡µé¢ç»“æ„
	log.Debug().Str("url", task.URL).Msg("ğŸ”¬ æ­£åœ¨åˆ†æé¡µé¢ç»“æ„ (Analyzing page structure)")
	pageStructure, err := o.analyzePageStructure(task.URL, bodyBytes)
	if err != nil {
		log.Error().Err(err).Str("url", task.URL).Msg("âŒ é¡µé¢ç»“æ„åˆ†æå¤±è´¥ (Failed to analyze page structure)")
		return
	}

	// 4. ç›¸ä¼¼åº¦æ£€æŸ¥
	if o.isSimilarPage(pageStructure) {
		log.Debug().Str("url", task.URL).Str("reason", "similar_page").Msg("â­ï¸ è·³è¿‡çˆ¬å– (Skipping crawl)")
		atomic.AddInt64(&o.stats.similarPagesSkipped, 1)
		return
	}

	// 5. ä¼ ç»Ÿå»é‡æ£€æŸ¥ï¼ˆä½œä¸ºå¤‡ä»½ï¼‰
	isUnique, err := o.deduplicator.IsUnique(task.URL, bytes.NewReader(bodyBytes))
	if err != nil {
		log.Error().Err(err).Str("url", task.URL).Msg("âŒ å»é‡æ£€æŸ¥å¤±è´¥ (Deduplication check failed)")
		return
	}
	if !isUnique {
		log.Debug().Str("url", task.URL).Str("reason", "duplicate_content").Msg("â­ï¸ è·³è¿‡çˆ¬å– (Skipping crawl)")
		reporter.LogDeDuplicateURL(task.URL)
		atomic.AddInt64(&o.stats.duplicatesSkipped, 1)
		return
	}

	// 6. å­˜å‚¨é¡µé¢ç»“æ„
	o.pageStructures.Store(task.URL, pageStructure)
	o.updateDomainStatistics(task.URL, pageStructure)

	// 7. çˆ¬å–å’Œè§£æé¡µé¢å†…å®¹
	log.Info().Str("url", task.URL).Msg("ğŸ å¼€å§‹é™æ€çˆ¬å– (Starting static crawl)")
	staticLinks, staticRequests, err := o.crawler.StaticCrawl(o.ctx, task.URL, bodyBytes)
	if err != nil {
		log.Error().Err(err).Str("url", task.URL).Msg("âŒ é™æ€çˆ¬å–å¤±è´¥ (Static crawl failed)")
		// å³ä½¿é™æ€çˆ¬å–å¤±è´¥ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥å°è¯•åŠ¨æ€çˆ¬å–
	} else {
		log.Info().
			Str("url", task.URL).
			Int("found_links", len(staticLinks)).
			Int("found_requests", len(staticRequests)).
			Msg("âœ… é™æ€çˆ¬å–å®Œæˆ (Static crawl finished)")
	}

	var allLinks []string
	var allRequests []*models.Request
	allLinks = append(allLinks, staticLinks...)
	allRequests = append(allRequests, staticRequests...)

	// å¦‚æœå¯ç”¨äº†åŠ¨æ€çˆ¬è™«ï¼Œåˆ™æ‰§è¡Œ
	if o.config.Spider.DynamicCrawler.Enabled {
		log.Info().Str("url", task.URL).Msg("ğŸ å¼€å§‹åŠ¨æ€çˆ¬å– (Starting dynamic crawl)")
		dynamicLinks, dynamicRequests, err := o.crawler.DynamicCrawl(o.ctx, task.URL)
		if err != nil {
			log.Error().Err(err).Str("url", task.URL).Msg("âŒ åŠ¨æ€çˆ¬å–å¤±è´¥ (Dynamic crawl failed)")
		} else {
			log.Info().
				Str("url", task.URL).
				Int("found_links", len(dynamicLinks)).
				Int("found_requests", len(dynamicRequests)).
				Msg("âœ… åŠ¨æ€çˆ¬å–å®Œæˆ (Dynamic crawl finished)")

			// åˆå¹¶åŠ¨æ€çˆ¬å–çš„ç»“æœ
			allLinks = append(allLinks, dynamicLinks...)
			allRequests = append(allRequests, dynamicRequests...)
		}
	}

	reporter.LogURL(task.URL)
	atomic.AddInt64(&o.stats.urlsProcessed, 1)
	log.Debug().Str("url", task.URL).Int("found_links", len(allLinks)).Int("found_requests", len(allRequests)).Msg("ğŸ”— å‘ç°æ–°é“¾æ¥å’Œè¯·æ±‚ (Found new links and requests)")

	// 8. è¿‡æ»¤å’ŒéªŒè¯æ–°å‘ç°çš„é“¾æ¥å’Œè¯·æ±‚
	validLinks := o.filterValidLinks(allLinks)
	validRequests := o.filterValidRequests(allRequests)
	log.Debug().Str("url", task.URL).Int("valid_links", len(validLinks)).Int("valid_requests", len(validRequests)).Msg("ğŸ›¡ï¸ è¿‡æ»¤åæœ‰æ•ˆçš„é“¾æ¥å’Œè¯·æ±‚ (Filtered valid links and requests)")

	// 9. ä¼˜å…ˆå¤„ç†ç»“æ„å·®å¼‚è¾ƒå¤§çš„è¡¨å•
	validRequests = o.prioritizeUniqueFormRequests(validRequests)

	// 10. å°†æ–°ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—
	totalTasks := len(validLinks) + len(validRequests)
	if totalTasks > 0 {
		wg.Add(totalTasks)
		log.Debug().Str("url", task.URL).Int("new_tasks", totalTasks).Msg("â• æ·»åŠ æ–°ä»»åŠ¡åˆ°é˜Ÿåˆ— (Adding new tasks to queue)")

		for _, link := range validLinks {
			select {
			case taskQueue <- models.Task{URL: link, Depth: task.Depth + 1}:
			case <-o.ctx.Done():
				wg.Done()
				return
			}
		}

		for _, req := range validRequests {
			select {
			case taskQueue <- models.Task{Request: req}:
			case <-o.ctx.Done():
				wg.Done()
				return
			}
		}
	}
}

// analyzePageStructure åˆ†æé¡µé¢ç»“æ„
func (o *Orchestrator) analyzePageStructure(pageURL string, bodyBytes []byte) (*PageStructure, error) {
	doc, err := html.Parse(bytes.NewReader(bodyBytes))
	if err != nil {
		return nil, fmt.Errorf("failed to parse HTML: %w", err)
	}

	structure := &PageStructure{
		FormFields: make(map[string]string),
	}

	// åˆ†æDOMç»“æ„
	structure.DOMHash = o.calculateDOMHash(doc)

	// åˆ†ææ–‡æœ¬å†…å®¹
	structure.TextHash = o.calculateTextHash(bodyBytes)

	// åˆ†æè¡¨å•ç»“æ„
	o.analyzeFormStructure(doc, structure)

	// ç»Ÿè®¡å„ç§å…ƒç´ 
	o.countElements(doc, structure)

	// æå–æ ‡é¢˜
	structure.Title = o.extractTitle(doc)

	return structure, nil
}

// calculateDOMHash è®¡ç®—DOMç»“æ„å“ˆå¸Œ
func (o *Orchestrator) calculateDOMHash(node *html.Node) string {
	var domStructure strings.Builder
	o.traverseDOM(node, &domStructure, 0)

	hash := md5.Sum([]byte(domStructure.String()))
	return fmt.Sprintf("%x", hash)
}

// traverseDOM éå†DOMç»“æ„
func (o *Orchestrator) traverseDOM(node *html.Node, builder *strings.Builder, depth int) {
	if node.Type == html.ElementNode {
		builder.WriteString(strings.Repeat("  ", depth))
		builder.WriteString(node.Data)

		// åŒ…å«é‡è¦å±æ€§
		for _, attr := range node.Attr {
			if attr.Key == "class" || attr.Key == "id" || attr.Key == "name" {
				builder.WriteString(fmt.Sprintf("[%s=%s]", attr.Key, attr.Val))
			}
		}
		builder.WriteString("\n")
	}

	for child := node.FirstChild; child != nil; child = child.NextSibling {
		o.traverseDOM(child, builder, depth+1)
	}
}

// calculateTextHash è®¡ç®—æ–‡æœ¬å†…å®¹å“ˆå¸Œ
func (o *Orchestrator) calculateTextHash(bodyBytes []byte) string {
	// æå–çº¯æ–‡æœ¬å†…å®¹
	text := string(bodyBytes)
	// ç§»é™¤HTMLæ ‡ç­¾
	re := regexp.MustCompile(`<[^>]*>`)
	text = re.ReplaceAllString(text, "")
	// ç§»é™¤å¤šä½™ç©ºç™½
	text = regexp.MustCompile(`\s+`).ReplaceAllString(text, " ")
	text = strings.TrimSpace(text)

	hash := md5.Sum([]byte(text))
	return fmt.Sprintf("%x", hash)
}

// analyzeFormStructure åˆ†æè¡¨å•ç»“æ„
func (o *Orchestrator) analyzeFormStructure(node *html.Node, structure *PageStructure) {
	if node.Type == html.ElementNode {
		switch node.Data {
		case "form":
			formStruct := o.extractFormStructure(node)
			if formStruct != nil {
				structure.FormFields[formStruct.Hash] = formStruct.Action
			}
		case "input", "textarea", "select":
			structure.InputCount++
		}
	}

	for child := node.FirstChild; child != nil; child = child.NextSibling {
		o.analyzeFormStructure(child, structure)
	}
}

// extractFormStructure æå–è¡¨å•ç»“æ„
func (o *Orchestrator) extractFormStructure(formNode *html.Node) *FormStructure {
	form := &FormStructure{
		Fields: make([]string, 0),
		Types:  make([]string, 0),
	}

	// æå–è¡¨å•å±æ€§
	for _, attr := range formNode.Attr {
		switch attr.Key {
		case "action":
			form.Action = attr.Val
		case "method":
			form.Method = attr.Val
		}
	}

	// æå–è¡¨å•å­—æ®µ
	o.extractFormFields(formNode, form)

	// è®¡ç®—è¡¨å•å“ˆå¸Œ
	form.Hash = o.calculateFormHash(form)

	return form
}

// extractFormFields æå–è¡¨å•å­—æ®µ
func (o *Orchestrator) extractFormFields(node *html.Node, form *FormStructure) {
	if node.Type == html.ElementNode {
		switch node.Data {
		case "input", "textarea", "select":
			var name, fieldType string
			for _, attr := range node.Attr {
				switch attr.Key {
				case "name":
					name = attr.Val
				case "type":
					fieldType = attr.Val
				}
			}
			if name != "" {
				form.Fields = append(form.Fields, name)
				form.Types = append(form.Types, fieldType)
			}
		}
	}

	for child := node.FirstChild; child != nil; child = child.NextSibling {
		o.extractFormFields(child, form)
	}
}

// calculateFormHash è®¡ç®—è¡¨å•ç»“æ„å“ˆå¸Œ
func (o *Orchestrator) calculateFormHash(form *FormStructure) string {
	var hashBuilder strings.Builder

	// æ’åºå­—æ®µåä»¥ç¡®ä¿ä¸€è‡´æ€§
	sortedFields := make([]string, len(form.Fields))
	copy(sortedFields, form.Fields)
	sort.Strings(sortedFields)

	for _, field := range sortedFields {
		hashBuilder.WriteString(field)
		hashBuilder.WriteString(":")
	}

	hash := md5.Sum([]byte(hashBuilder.String()))
	return fmt.Sprintf("%x", hash)
}

// countElements ç»Ÿè®¡é¡µé¢å…ƒç´ 
func (o *Orchestrator) countElements(node *html.Node, structure *PageStructure) {
	if node.Type == html.ElementNode {
		switch node.Data {
		case "a":
			structure.LinkCount++
		case "script":
			structure.ScriptCount++
		}
	}

	for child := node.FirstChild; child != nil; child = child.NextSibling {
		o.countElements(child, structure)
	}
}

// extractTitle æå–é¡µé¢æ ‡é¢˜
func (o *Orchestrator) extractTitle(node *html.Node) string {
	if node.Type == html.ElementNode && node.Data == "title" {
		if node.FirstChild != nil && node.FirstChild.Type == html.TextNode {
			return strings.TrimSpace(node.FirstChild.Data)
		}
	}

	for child := node.FirstChild; child != nil; child = child.NextSibling {
		if title := o.extractTitle(child); title != "" {
			return title
		}
	}

	return ""
}

// isURLPatternDuplicate æ£€æŸ¥URLæ¨¡å¼æ˜¯å¦é‡å¤
func (o *Orchestrator) isURLPatternDuplicate(targetURL string) bool {
	pattern := o.extractURLPattern(targetURL)
	if pattern == "" {
		return false
	}

	_, exists := o.urlPatterns.LoadOrStore(pattern, URLPattern{
		BaseURL: targetURL,
		Pattern: pattern,
	})

	return exists
}

// extractURLPattern æå–URLæ¨¡å¼
func (o *Orchestrator) extractURLPattern(targetURL string) string {
	parsedURL, err := url.Parse(targetURL)
	if err != nil {
		return ""
	}

	// å°†æ•°å­—å‚æ•°å€¼æ›¿æ¢ä¸ºå ä½ç¬¦
	query := parsedURL.Query()
	var paramNames []string

	for key, values := range query {
		paramNames = append(paramNames, key)
		// æ£€æŸ¥å€¼æ˜¯å¦ä¸ºæ•°å­—
		for i, value := range values {
			if _, err := strconv.Atoi(value); err == nil {
				values[i] = "{num}"
			}
		}
		query[key] = values
	}

	sort.Strings(paramNames)
	parsedURL.RawQuery = query.Encode()

	return parsedURL.String()
}

// isSimilarPage æ£€æŸ¥é¡µé¢æ˜¯å¦ç›¸ä¼¼
func (o *Orchestrator) isSimilarPage(newStructure *PageStructure) bool {
	var maxSimilarity float64

	o.pageStructures.Range(func(key, value interface{}) bool {
		existingStructure := value.(*PageStructure)

		// è®¡ç®—DOMç›¸ä¼¼åº¦
		domSimilarity := o.calculateDOMSimilarity(newStructure.DOMHash, existingStructure.DOMHash)

		// è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦
		contentSimilarity := o.calculateContentSimilarity(newStructure.TextHash, existingStructure.TextHash)

		// è®¡ç®—è¡¨å•ç›¸ä¼¼åº¦
		formSimilarity := o.calculateFormSimilarity(newStructure.FormFields, existingStructure.FormFields)

		// ç»¼åˆç›¸ä¼¼åº¦
		overallSimilarity := (domSimilarity + contentSimilarity + formSimilarity) / 3.0

		if overallSimilarity > maxSimilarity {
			maxSimilarity = overallSimilarity
		}

		return true // ç»§ç»­éå†
	})

	// æ£€æŸ¥æ˜¯å¦è¶…è¿‡é˜ˆå€¼
	return maxSimilarity > o.similarityConfig.DOMThreshold
}

// calculateDOMSimilarity è®¡ç®—DOMç»“æ„ç›¸ä¼¼åº¦
func (o *Orchestrator) calculateDOMSimilarity(hash1, hash2 string) float64 {
	if hash1 == hash2 {
		return 1.0
	}

	// ä½¿ç”¨Jaccardç›¸ä¼¼åº¦è®¡ç®—
	// è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•
	return o.calculateHashSimilarity(hash1, hash2)
}

// calculateContentSimilarity è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦
func (o *Orchestrator) calculateContentSimilarity(hash1, hash2 string) float64 {
	if hash1 == hash2 {
		return 1.0
	}

	return o.calculateHashSimilarity(hash1, hash2)
}

// calculateFormSimilarity è®¡ç®—è¡¨å•ç›¸ä¼¼åº¦
func (o *Orchestrator) calculateFormSimilarity(forms1, forms2 map[string]string) float64 {
	if len(forms1) == 0 && len(forms2) == 0 {
		return 1.0
	}

	if len(forms1) == 0 || len(forms2) == 0 {
		return 0.0
	}

	// è®¡ç®—è¡¨å•å­—æ®µçš„äº¤é›†å’Œå¹¶é›†
	intersection := 0
	union := len(forms1)

	for hash1 := range forms1 {
		if _, exists := forms2[hash1]; exists {
			intersection++
		}
	}

	for hash2 := range forms2 {
		if _, exists := forms1[hash2]; !exists {
			union++
		}
	}

	if union == 0 {
		return 1.0
	}

	return float64(intersection) / float64(union)
}

// calculateHashSimilarity è®¡ç®—å“ˆå¸Œç›¸ä¼¼åº¦
func (o *Orchestrator) calculateHashSimilarity(hash1, hash2 string) float64 {
	if len(hash1) != len(hash2) {
		return 0.0
	}

	matches := 0
	for i := 0; i < len(hash1); i++ {
		if hash1[i] == hash2[i] {
			matches++
		}
	}

	return float64(matches) / float64(len(hash1))
}

// updateDomainStatistics æ›´æ–°åŸŸåç»Ÿè®¡ä¿¡æ¯
func (o *Orchestrator) updateDomainStatistics(pageURL string, structure *PageStructure) {
	parsedURL, err := url.Parse(pageURL)
	if err != nil {
		return
	}

	domain := parsedURL.Host

	o.domainStatsMutex.Lock()
	defer o.domainStatsMutex.Unlock()

	stats, exists := o.domainStats[domain]
	if !exists {
		stats = &DomainStatistics{
			LastAdjustment: time.Now(),
		}
		o.domainStats[domain] = stats
	}

	stats.TotalPages++
	stats.UniqueForms += len(structure.FormFields)

	// è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–å¤„ç†ï¼‰
	if stats.TotalPages > 1 {
		// è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„å¹³å‡ç›¸ä¼¼åº¦è®¡ç®—
		stats.AverageSimilarity = (stats.AverageSimilarity*float64(stats.TotalPages-1) + 0.5) / float64(stats.TotalPages)
	}
}

// prioritizeUniqueFormRequests ä¼˜å…ˆå¤„ç†ç»“æ„å·®å¼‚è¾ƒå¤§çš„è¡¨å•è¯·æ±‚
func (o *Orchestrator) prioritizeUniqueFormRequests(requests []*models.Request) []*models.Request {
	if len(requests) <= 1 {
		return requests
	}

	// æŒ‰è¡¨å•å”¯ä¸€æ€§æ’åº
	sort.Slice(requests, func(i, j int) bool {
		scoreI := o.calculateFormUniquenessScore(requests[i])
		scoreJ := o.calculateFormUniquenessScore(requests[j])
		return scoreI > scoreJ // åˆ†æ•°é«˜çš„æ’åœ¨å‰é¢
	})

	return requests
}

// calculateFormUniquenessScore è®¡ç®—è¡¨å•å”¯ä¸€æ€§åˆ†æ•°
func (o *Orchestrator) calculateFormUniquenessScore(req *models.Request) float64 {
	if len(req.Params) == 0 {
		return 0.0
	}

	// åˆ›å»ºè¡¨å•ç»“æ„å“ˆå¸Œ
	var formBuilder strings.Builder
	paramNames := make([]string, 0, len(req.Params))

	for _, param := range req.Params {
		paramNames = append(paramNames, param.Name)
	}

	sort.Strings(paramNames)
	for _, name := range paramNames {
		formBuilder.WriteString(name)
		formBuilder.WriteString(":")
	}

	formHash := fmt.Sprintf("%x", md5.Sum([]byte(formBuilder.String())))

	// æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸ä¼¼è¡¨å•
	similarityCount := 0
	o.formStructures.Range(func(key, value interface{}) bool {
		existingHash := key.(string)
		similarity := o.calculateHashSimilarity(formHash, existingHash)
		if similarity > o.similarityConfig.FormThreshold {
			similarityCount++
		}
		return true
	})

	// å­˜å‚¨è¡¨å•ç»“æ„
	o.formStructures.Store(formHash, true)

	// è¿”å›å”¯ä¸€æ€§åˆ†æ•°ï¼ˆç›¸ä¼¼è¡¨å•è¶Šå°‘ï¼Œåˆ†æ•°è¶Šé«˜ï¼‰
	return 1.0 / (1.0 + float64(similarityCount))
}

// fetchURLWithRetry å¸¦é‡è¯•æœºåˆ¶çš„URLè·å–
func (o *Orchestrator) fetchURLWithRetry(url string) ([]byte, error) {
	var lastErr error

	for attempt := 0; attempt <= o.retryConfig.maxRetries; attempt++ {
		if attempt > 0 {
			log.Debug().Str("url", url).Int("attempt", attempt).Msg("Retrying URL fetch")
			time.Sleep(o.retryConfig.retryDelay)
		}

		resp, err := o.httpClient.Get(o.ctx, url, nil)
		if err != nil {
			lastErr = err
			if !o.isRetryableError(err) {
				break
			}
			continue
		}

		bodyBytes, err := io.ReadAll(resp.Body)
		resp.Body.Close()

		if err != nil {
			lastErr = err
			continue
		}

		return bodyBytes, nil
	}

	return nil, fmt.Errorf("failed after %d attempts: %w", o.retryConfig.maxRetries+1, lastErr)
}

// isRetryableError åˆ¤æ–­é”™è¯¯æ˜¯å¦å¯é‡è¯•
func (o *Orchestrator) isRetryableError(err error) bool {
	errStr := err.Error()
	retryableErrors := []string{
		"timeout",
		"connection reset",
		"connection refused",
		"temporary failure",
		"server closed",
	}

	for _, retryable := range retryableErrors {
		if strings.Contains(strings.ToLower(errStr), retryable) {
			return true
		}
	}

	return false
}

// filterValidLinks è¿‡æ»¤æœ‰æ•ˆçš„é“¾æ¥
func (o *Orchestrator) filterValidLinks(links []string) []string {
	var validLinks []string

	for _, link := range links {
		if link == "" || len(link) > 2048 {
			continue
		}

		if o.isStaticResource(link) {
			continue
		}

		validLinks = append(validLinks, link)
	}

	return validLinks
}

// filterValidRequests è¿‡æ»¤æœ‰æ•ˆçš„è¯·æ±‚
func (o *Orchestrator) filterValidRequests(requests []*models.Request) []*models.Request {
	var validRequests []*models.Request

	for _, req := range requests {
		if req == nil || req.URL == nil {
			continue
		}

		if o.isStaticResource(req.URL.String()) {
			continue
		}

		if !o.isValidHTTPMethod(req.Method) {
			continue
		}

		validRequests = append(validRequests, req)
	}

	return validRequests
}

// isStaticResource åˆ¤æ–­æ˜¯å¦ä¸ºé™æ€èµ„æº
func (o *Orchestrator) isStaticResource(url string) bool {
	staticExtensions := []string{
		".css", ".js", ".png", ".jpg", ".jpeg", ".gif", ".ico", ".svg",
		".woff", ".woff2", ".ttf", ".eot", ".pdf", ".zip", ".tar", ".gz",
		".mp4", ".mp3", ".avi", ".mov", ".wmv", ".flv",
	}

	urlLower := strings.ToLower(url)
	for _, ext := range staticExtensions {
		if strings.HasSuffix(urlLower, ext) {
			return true
		}
	}

	return false
}

// isValidHTTPMethod éªŒè¯HTTPæ–¹æ³•æ˜¯å¦æœ‰æ•ˆ
func (o *Orchestrator) isValidHTTPMethod(method string) bool {
	validMethods := []string{
		http.MethodGet, http.MethodPost, http.MethodPut,
		http.MethodDelete, http.MethodPatch, http.MethodHead,
		http.MethodOptions,
	}

	for _, validMethod := range validMethods {
		if strings.EqualFold(method, validMethod) {
			return true
		}
	}

	return false
}

// scanRequestWithRetry å¯¹å•ä¸ªè¯·æ±‚æ‰§è¡Œæ‰«æï¼ˆåŒ…å«é‡è¯•é€»è¾‘ï¼‰ã€‚
func (o *Orchestrator) scanRequestWithRetry(ctx context.Context, req *models.Request, reporter *output.Reporter) {
	log.Info().Str("url", req.URL.String()).Msg("ğŸ å¼€å§‹æ¼æ´æ‰«æ (Starting vulnerability scan)")
	for attempt := 0; attempt <= o.retryConfig.maxRetries; attempt++ {
		if attempt > 0 {
			log.Debug().Str("url", req.URL.String()).Int("attempt", attempt).Msg("ğŸ” é‡è¯•è¯·æ±‚æ‰«æ (Retrying request scan)")
			time.Sleep(o.retryConfig.retryDelay)
		}

		vulnerabilities := o.scanRequest(ctx, req, reporter)
		if vulnerabilities > 0 {
			atomic.AddInt64(&o.stats.vulnerabilitiesFound, int64(vulnerabilities))
		}

		return // æ— è®ºæˆåŠŸä¸å¦ï¼Œåªæ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„æ‰«ææµç¨‹
	}
	log.Error().Str("url", req.URL.String()).Msg("âŒ æ‰«æè¯·æ±‚å¤±è´¥ (Scan request failed after retries)")
}

// scanRequest å¯¹å•ä¸ªè¯·æ±‚æ‰§è¡Œæ‰€æœ‰æ’ä»¶çš„æ‰«æï¼Œå¹¶æŠ¥å‘Šå‘ç°çš„æ¼æ´ã€‚
func (o *Orchestrator) scanRequest(ctx context.Context, req *models.Request, reporter *output.Reporter) int {
	// ä½¿ç”¨æ‰«æå¼•æ“æ‰§è¡Œæ‰«æ
	vulnerabilities := o.scanEngine.Execute(req)

	// å¦‚æœAIåˆ†æå™¨å¯ç”¨ï¼Œå¯ä»¥æ·»åŠ é¢å¤–çš„åˆ†æé€»è¾‘
	if o.aiAnalyzer != nil && len(vulnerabilities) > 0 {
		// ä¾‹å¦‚ï¼šè®©AIå¯¹å‘ç°çš„æ¼æ´è¿›è¡ŒäºŒæ¬¡éªŒè¯æˆ–åˆ†æ
		log.Debug().Int("count", len(vulnerabilities)).Msg("ğŸ¤– å°†å‘ç°çš„æ¼æ´æäº¤ç»™AIè¿›è¡Œåˆ†æ... (Submitting vulnerabilities to AI for analysis...)")
	}

	for _, vuln := range vulnerabilities {
		reporter.LogVulnerability(vuln)
	}

	if len(vulnerabilities) > 0 {
		log.Info().Int("count", len(vulnerabilities)).Str("url", req.URL.String()).Msg("ğŸš¨ å‘ç°æ–°æ¼æ´ï¼ (New vulnerabilities found!)")
	}

	return len(vulnerabilities)
}
