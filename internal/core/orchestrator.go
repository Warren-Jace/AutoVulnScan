// Package core åŒ…å«äº† AutoVulnScan åº”ç”¨ç¨‹åºçš„æ ¸å¿ƒç¼–æ’å™¨ã€‚
package core

import (
	"bytes"
	"context"
	"crypto/md5"
	"errors"
	"fmt"
	"io"
	"net/http"
	"net/url"
	"regexp"
	"sort"
	"strconv"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"autovulnscan/internal/ai"
	"autovulnscan/internal/browser"
	"autovulnscan/internal/config"
	"autovulnscan/internal/crawler"
	"autovulnscan/internal/dedup"
	"autovulnscan/internal/models"
	"autovulnscan/internal/output"
	"autovulnscan/internal/requester"
	"autovulnscan/internal/vulnscan"
	_ "autovulnscan/internal/vulnscan/plugins" // åŒ¿åå¯¼å…¥ä»¥æ‰§è¡Œæ’ä»¶çš„init()å‡½æ•°è¿›è¡Œæ³¨å†Œ

	"github.com/rs/zerolog/log"
	"golang.org/x/net/html"
)

// PageStructure é¡µé¢ç»“æ„ä¿¡æ¯
type PageStructure struct {
	DOMHash     string            // DOMç»“æ„å“ˆå¸Œ
	TextHash    string            // æ–‡æœ¬å†…å®¹å“ˆå¸Œ
	FormFields  map[string]string // è¡¨å•å­—æ®µæ˜ å°„
	InputCount  int               // è¾“å…¥å­—æ®µæ•°é‡
	LinkCount   int               // é“¾æ¥æ•°é‡
	ScriptCount int               // è„šæœ¬æ•°é‡
	Title       string            // é¡µé¢æ ‡é¢˜
}

// TimestampedPageStructure å¸¦æ—¶é—´æˆ³çš„é¡µé¢ç»“æ„
type TimestampedPageStructure struct {
	*PageStructure
	Timestamp time.Time
}

// URLPattern URLæ¨¡å¼
type URLPattern struct {
	BaseURL    string   // åŸºç¡€URL
	ParamNames []string // å‚æ•°ååˆ—è¡¨
	Pattern    string   // URLæ¨¡å¼
}

// SimilarityConfig ç›¸ä¼¼åº¦é…ç½®
type SimilarityConfig struct {
	DOMThreshold     float64 // DOMç»“æ„ç›¸ä¼¼åº¦é˜ˆå€¼
	ContentThreshold float64 // å†…å®¹ç›¸ä¼¼åº¦é˜ˆå€¼
	FormThreshold    float64 // è¡¨å•ç›¸ä¼¼åº¦é˜ˆå€¼
	URLThreshold     float64 // URLæ¨¡å¼ç›¸ä¼¼åº¦é˜ˆå€¼
	AutoAdjust       bool    // æ˜¯å¦è‡ªåŠ¨è°ƒæ•´é˜ˆå€¼
}

// Orchestrator è´Ÿè´£åè°ƒçˆ¬è™«ã€æ‰«æå’ŒæŠ¥å‘Šçš„ä¸»æµç¨‹æ§åˆ¶å™¨ã€‚
type Orchestrator struct {
	config       *config.Settings
	targetURL    string
	crawler      *crawler.Crawler
	scanEngine   *vulnscan.Engine // ä½¿ç”¨æ‰«æå¼•æ“
	deduplicator *dedup.Deduplicator
	aiAnalyzer   *ai.AIAnalyzer
	httpClient   *requester.HTTPClient
	ctx          context.Context
	cancel       context.CancelFunc

	vulnerabilities []*vulnscan.Vulnerability // å­˜å‚¨æ‰€æœ‰å‘ç°çš„æ¼æ´
	vulnMutex       sync.Mutex                // ä¿æŠ¤vulnerabilitiesåˆ‡ç‰‡

	stats struct {
		urlsProcessed        int64
		requestsScanned      int64
		paramsFound          int64
		postParamsFound      int64
		vulnerabilitiesFound int64
		duplicatesSkipped    int64
		similarPagesSkipped  int64
		startTime            time.Time
		currentPhase         string
	}

	retryConfig struct {
		maxRetries int
		retryDelay time.Duration
	}

	similarityConfig SimilarityConfig
	pageStructures   sync.Map
	urlPatterns      sync.Map
	formStructures   sync.Map
	requestDedup     sync.Map
	domainStats      map[string]*DomainStatistics
	domainStatsMutex sync.RWMutex

	// æ¸…ç†ç›¸å…³
	cleanupTicker *time.Ticker
	cleanupDone   chan struct{}
}

// DomainStatistics åŸŸåç»Ÿè®¡ä¿¡æ¯ï¼Œç”¨äºåŠ¨æ€è°ƒæ•´é˜ˆå€¼
type DomainStatistics struct {
	TotalPages        int       // æ€»é¡µé¢æ•°
	UniqueForms       int       // å”¯ä¸€è¡¨å•æ•°
	AverageSimilarity float64   // å¹³å‡ç›¸ä¼¼åº¦
	LastAdjustment    time.Time // æœ€åè°ƒæ•´æ—¶é—´
}

// FormStructure è¡¨å•ç»“æ„
type FormStructure struct {
	Fields []string // å­—æ®µååˆ—è¡¨
	Types  []string // å­—æ®µç±»å‹åˆ—è¡¨
	Action string   // è¡¨å•action
	Method string   // è¡¨å•method
	Hash   string   // ç»“æ„å“ˆå¸Œ
}

// å¯¹è±¡æ± ä¼˜åŒ–
var builderPool = sync.Pool{
	New: func() interface{} {
		return &strings.Builder{}
	},
}

// validateConfig éªŒè¯é…ç½®
func validateConfig(cfg *config.Settings) error {
	if cfg == nil {
		return errors.New("é…ç½®ä¸èƒ½ä¸ºç©º")
	}
	if cfg.Spider.Concurrency <= 0 {
		return errors.New("çˆ¬è™«å¹¶å‘æ•°å¿…é¡»å¤§äº0")
	}
	if cfg.Spider.MaxDepth < 0 {
		return errors.New("æœ€å¤§æ·±åº¦ä¸èƒ½ä¸ºè´Ÿæ•°")
	}
	if cfg.Scanner.Timeout <= 0 {
		return errors.New("æ‰«æå™¨è¶…æ—¶æ—¶é—´å¿…é¡»å¤§äº0")
	}
	return nil
}

// validateTargetURL éªŒè¯ç›®æ ‡URL
func validateTargetURL(targetURL string) error {
	if targetURL == "" {
		return errors.New("ç›®æ ‡URLä¸èƒ½ä¸ºç©º")
	}

	parsedURL, err := url.Parse(targetURL)
	if err != nil {
		return fmt.Errorf("æ— æ•ˆçš„URLæ ¼å¼: %w", err)
	}

	if parsedURL.Scheme != "http" && parsedURL.Scheme != "https" {
		return errors.New("URLå¿…é¡»ä½¿ç”¨httpæˆ–httpsåè®®")
	}

	if parsedURL.Host == "" {
		return errors.New("URLå¿…é¡»åŒ…å«æœ‰æ•ˆçš„ä¸»æœºå")
	}

	return nil
}

// NewOrchestrator åˆ›å»ºå¹¶åˆå§‹åŒ–ä¸€ä¸ªOrchestratorå®ä¾‹ã€‚
// è¿™ä¸ªå‡½æ•°è´Ÿè´£ç»„è£…æ‰€æœ‰å¿…è¦çš„ç»„ä»¶ï¼Œå¦‚HTTPå®¢æˆ·ç«¯ã€çˆ¬è™«ã€æ‰«æå¼•æ“ç­‰ã€‚
func NewOrchestrator(cfg *config.Settings, targetURL string) (*Orchestrator, error) {
	// éªŒè¯è¾“å…¥å‚æ•°
	if err := validateConfig(cfg); err != nil {
		return nil, fmt.Errorf("é…ç½®éªŒè¯å¤±è´¥: %w", err)
	}

	if err := validateTargetURL(targetURL); err != nil {
		return nil, fmt.Errorf("ç›®æ ‡URLéªŒè¯å¤±è´¥: %w", err)
	}

	ctx, cancel := context.WithCancel(context.Background())

	// ä¸ºçˆ¬è™«åˆ›å»ºç‹¬ç«‹çš„HTTPå®¢æˆ·ç«¯
	spiderHttpClient := requester.NewHTTPClient(cfg.Spider.Timeout, cfg.Proxy, cfg.Headers)

	cr, err := crawler.NewCrawler(targetURL, cfg, spiderHttpClient)
	if err != nil {
		cancel()
		return nil, fmt.Errorf("åˆå§‹åŒ–çˆ¬è™«å¤±è´¥: %w", err)
	}

	// ä¸ºæ‰«æå™¨åˆ›å»ºç‹¬ç«‹çš„HTTPå®¢æˆ·ç«¯
	scannerHttpClient := requester.NewHTTPClient(int(cfg.Scanner.Timeout/time.Second), cfg.Proxy, cfg.Headers)

	// åˆå§‹åŒ–æµè§ˆå™¨æœåŠ¡
	var browserService *browser.BrowserService
	if cfg.Spider.DynamicCrawler.Enabled {
		browserService, err = browser.NewBrowserService(browser.Config{
			Headless:  cfg.Spider.DynamicCrawler.Headless,
			Proxy:     cfg.Proxy,
			UserAgent: cfg.Headers["User-Agent"],
		})
		if err != nil {
			log.Warn().Err(err).Msg("åˆå§‹åŒ–æµè§ˆå™¨æœåŠ¡å¤±è´¥ï¼Œéƒ¨åˆ†åŠŸèƒ½ï¼ˆå¦‚XSS DOMéªŒè¯ï¼‰å°†å—é™")
			// éè‡´å‘½é”™è¯¯ï¼Œå…è®¸ç»§ç»­
		}
	}

	scanEngine, err := vulnscan.NewEngine(&cfg.Scanner, scannerHttpClient, browserService)
	if err != nil {
		cancel()
		return nil, fmt.Errorf("åˆå§‹åŒ–æ‰«æå¼•æ“å¤±è´¥: %w", err)
	}

	var aiAnalyzer *ai.AIAnalyzer
	if cfg.AIModule.Enabled {
		aiAnalyzer, err = ai.NewAIAnalyzer(cfg.AIModule.APIKey, cfg.AIModule.Model, "")
		if err != nil {
			log.Warn().Err(err).Msg("åˆå§‹åŒ–AIåˆ†æå™¨å¤±è´¥ï¼ŒAIåŠŸèƒ½å°†è¢«ç¦ç”¨")
		}
	}

	o := &Orchestrator{
		config:       cfg,
		targetURL:    targetURL,
		crawler:      cr,
		scanEngine:   scanEngine,
		deduplicator: dedup.NewDeduplicator(dedup.WithThreshold(0.95)),
		aiAnalyzer:   aiAnalyzer,
		httpClient:   spiderHttpClient, // Orchestratorè‡ªèº«ä¿ç•™ä¸€ä¸ªç”¨äºé€šç”¨ç›®çš„çš„å®¢æˆ·ç«¯
		ctx:          ctx,
		cancel:       cancel,
		domainStats:  make(map[string]*DomainStatistics),
		cleanupDone:  make(chan struct{}),
	}

	// åˆå§‹åŒ–ç»Ÿè®¡æ•°æ®
	o.stats.startTime = time.Now()
	o.stats.currentPhase = "åˆå§‹åŒ–"

	// åˆå§‹åŒ–é‡è¯•é…ç½®
	o.retryConfig.maxRetries = 3
	o.retryConfig.retryDelay = 2 * time.Second

	// åˆå§‹åŒ–ç›¸ä¼¼åº¦é…ç½®
	o.initSimilarityConfig()

	// å¯åŠ¨æ¸…ç†ä»»åŠ¡
	o.startCleanupTask()

	return o, nil
}

// startCleanupTask å¯åŠ¨æ¸…ç†ä»»åŠ¡
func (o *Orchestrator) startCleanupTask() {
	o.cleanupTicker = time.NewTicker(30 * time.Minute) // æ¯30åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
	go func() {
		defer o.cleanupTicker.Stop()
		for {
			select {
			case <-o.cleanupTicker.C:
				o.cleanupOldStructures()
			case <-o.cleanupDone:
				return
			case <-o.ctx.Done():
				return
			}
		}
	}()
}

// stopCleanupTask åœæ­¢æ¸…ç†ä»»åŠ¡
func (o *Orchestrator) stopCleanupTask() {
	if o.cleanupTicker != nil {
		close(o.cleanupDone)
	}
}

// cleanupOldStructures æ¸…ç†æ—§çš„é¡µé¢ç»“æ„
func (o *Orchestrator) cleanupOldStructures() {
	cutoff := time.Now().Add(-time.Hour)
	cleaned := 0

	o.pageStructures.Range(func(key, value interface{}) bool {
		if shouldCleanup(value, cutoff) {
			o.pageStructures.Delete(key)
			cleaned++
		}
		return true
	})

	if cleaned > 0 {
		log.Debug().Int("cleaned", cleaned).Msg("æ¸…ç†äº†è¿‡æœŸçš„é¡µé¢ç»“æ„")
	}
}

// shouldCleanup æ£€æŸ¥æ˜¯å¦åº”è¯¥æ¸…ç†
func shouldCleanup(value interface{}, cutoff time.Time) bool {
	if timestamped, ok := value.(*TimestampedPageStructure); ok {
		return timestamped.Timestamp.Before(cutoff)
	}
	return false
}

// isInScope æ£€æŸ¥ç»™å®šçš„URLæ˜¯å¦åœ¨æ‰«æèŒƒå›´å†…ã€‚
// å®ƒä¼šæ ¹æ®é…ç½®çš„åŸŸåèŒƒå›´å’Œé»‘åå•è¿›è¡Œåˆ¤æ–­ã€‚
func (o *Orchestrator) isInScope(link string) bool {
	parsedURL, err := url.Parse(link)
	if err != nil {
		log.Debug().Str("url", link).Err(err).Msg("æ— æ³•è§£æURLï¼Œå·²è·³è¿‡")
		return false
	}

	// æ£€æŸ¥URLæ˜¯å¦åœ¨é»‘åå•ä¸­
	for _, blacklistedPattern := range o.config.Blacklist {
		if matched, _ := regexp.MatchString(blacklistedPattern, link); matched {
			return false
		}
	}

	// æ£€æŸ¥URLåŸŸåæ˜¯å¦åœ¨èŒƒå›´å†…
	for _, scopeDomain := range o.config.Scope {
		if strings.HasSuffix(parsedURL.Host, scopeDomain) {
			return true
		}
	}

	return false
}

// initSimilarityConfig åˆå§‹åŒ–ç›¸ä¼¼åº¦é…ç½®
func (o *Orchestrator) initSimilarityConfig() {
	o.similarityConfig = SimilarityConfig{
		DOMThreshold:     0.85, // DOMç»“æ„ç›¸ä¼¼åº¦é˜ˆå€¼85%
		ContentThreshold: 0.80, // å†…å®¹ç›¸ä¼¼åº¦é˜ˆå€¼80%
		FormThreshold:    0.90, // è¡¨å•ç›¸ä¼¼åº¦é˜ˆå€¼90%
		URLThreshold:     0.75, // URLæ¨¡å¼ç›¸ä¼¼åº¦é˜ˆå€¼75%
		AutoAdjust:       false, // å¯ç”¨è‡ªåŠ¨è°ƒæ•´
	}
}

// Start å¯åŠ¨ç¼–æ’å™¨çš„æ€»æ‰§è¡Œæµç¨‹ã€‚
func (o *Orchestrator) Start(reporter *output.Reporter) {
	log.Info().Msg("ğŸš€ æ‰«æä»»åŠ¡å¼€å§‹ (Scan task started)")
	o.stats.startTime = time.Now()

	// ç¡®ä¿åœ¨ä»»åŠ¡ç»“æŸæ—¶å…³é—­æŠ¥å‘Šå™¨å’Œæ¸…ç†èµ„æº
	defer func() {
		reporter.Close()
		o.stopCleanupTask()
	}()

	// å¯åŠ¨ä¸€ä¸ªgoroutineæ¥æ”¶é›†æ¼æ´
	go o.collectVulnerabilities()

	// å¯åŠ¨ç»Ÿè®¡å’Œé˜ˆå€¼è°ƒæ•´çš„ Ticker
	statsTicker := time.NewTicker(30 * time.Second)
	defer statsTicker.Stop()

	if o.similarityConfig.AutoAdjust {
		adjustTicker := time.NewTicker(1 * time.Minute)
		defer adjustTicker.Stop()
		go o.autoAdjustThresholds(adjustTicker.C)
	}

	// --- é˜¶æ®µä¸€: çˆ¬å– ---
	o.stats.currentPhase = "æ­£åœ¨çˆ¬å–"
	requestsToScan, err := o.crawl(reporter)
	if err != nil {
		log.Error().Err(err).Msg("çˆ¬å–é˜¶æ®µå¤±è´¥")
		return
	}

	// --- é˜¶æ®µäºŒ: æ‰«æ ---
	o.stats.currentPhase = "æ¼æ´æ£€æµ‹ä¸­"
	o.scan(requestsToScan)

	o.cancel()
	o.scanEngine.Close()

	log.Info().Msg("âœ… æ‰«æä»»åŠ¡å®Œæˆ (Scan task finished)")
	o.printFinalStats()
}

func (o *Orchestrator) crawl(reporter *output.Reporter) ([]*models.Request, error) {
	log.Info().Msg("--- çˆ¬å–é˜¶æ®µå¼€å§‹ ---")
	var wg sync.WaitGroup
	crawlQueue := make(chan models.Task, o.config.Spider.Concurrency*2)
	seenURLs := &sync.Map{}
	var requestsToScan []*models.Request
	var reqMutex sync.Mutex

	// å¯åŠ¨çˆ¬è™«å·¥ä½œåç¨‹
	for i := 1; i <= o.config.Spider.Concurrency; i++ {
		go func(workerID int) {
			for task := range crawlQueue {
				o.handleCrawlTask(task, &wg, reporter, seenURLs, crawlQueue, &requestsToScan, &reqMutex)
				wg.Done()
			}
		}(i)
	}

	// æ·»åŠ å…¥å£URL
	wg.Add(1)
	crawlQueue <- models.Task{URL: o.targetURL, Depth: 0}

	wg.Wait()
	close(crawlQueue)
	log.Info().Msg("--- çˆ¬å–é˜¶æ®µå®Œæˆ ---")
	return requestsToScan, nil
}

func (o *Orchestrator) scan(requests []*models.Request) {
	log.Info().Int("request_count", len(requests)).Msg("--- æ‰«æé˜¶æ®µå¼€å§‹ ---")
	o.scanEngine.Start()

	for _, req := range requests {
		o.scanEngine.QueueRequest(req)
		atomic.AddInt64(&o.stats.requestsScanned, 1)
	}

	o.scanEngine.Stop()
	log.Info().Msg("--- æ‰«æé˜¶æ®µå®Œæˆ ---")
}

func (o *Orchestrator) collectVulnerabilities() {
	for vuln := range o.scanEngine.VulnerabilityChan() {
		atomic.AddInt64(&o.stats.vulnerabilitiesFound, 1)

		o.vulnMutex.Lock()
		o.vulnerabilities = append(o.vulnerabilities, vuln)
		o.vulnMutex.Unlock()

		mode := "normal"
		if o.config.Debug {
			mode = "debug"
		}

		log.Warn().
			Str("æ¨¡å¼", mode).
			Str("URL", vuln.URL).
			Str("æ–¹æ³•", vuln.Method).
			Str("å‚æ•°", vuln.Param).
			Str("Payload", vuln.Payload).
			Msgf("ğŸš¨ å‘ç°æ¼æ´ (Vulnerability found)")
	}
}

// printFinalStats è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
func (o *Orchestrator) printFinalStats() {
	totalTime := time.Since(o.stats.startTime).Round(time.Second)
	mode := "normal"
	if o.config.Debug {
		mode = "debug"
	}
	log.Info().Msgf(`\n======== ğŸ“ˆ æ‰«æç»Ÿè®¡æ±‡æ€» ğŸ“ˆ ========\n| æ€»ç”¨æ—¶:           %s\n| å·²å¤„ç†URLæ•°:      %d\n| å·²æ‰«æè¯·æ±‚æ•°:     %d\n| å·²å‘ç°å‚æ•°æ•°:     %d\n| å·²å‘ç°POSTå‚æ•°æ•°: %d\n| å·²å‘ç°æ¼æ´æ•°:     %d\n| è·³è¿‡é‡å¤URLæ•°:    %d\n| è·³è¿‡ç›¸ä¼¼é¡µé¢æ•°:   %d\n| å½“å‰æ¨¡å¼:         %s\n| æ—¥å¿—æ–‡ä»¶:         %s\n| æŠ¥å‘Šæ–‡ä»¶è·¯å¾„:     %s\n====================================`,
		totalTime,
		atomic.LoadInt64(&o.stats.urlsProcessed),
		atomic.LoadInt64(&o.stats.requestsScanned),
		atomic.LoadInt64(&o.stats.paramsFound),
		atomic.LoadInt64(&o.stats.postParamsFound),
		atomic.LoadInt64(&o.stats.vulnerabilitiesFound),
		atomic.LoadInt64(&o.stats.duplicatesSkipped),
		atomic.LoadInt64(&o.stats.similarPagesSkipped),
		mode,
		o.config.Log.FilePath,
		o.config.Reporting.Path,
	)

	// è¾“å‡ºåŸŸåç»Ÿè®¡
	o.domainStatsMutex.RLock()
	for domain, stats := range o.domainStats {
		log.Info().
			Str("domain", domain).
			Int("total_pages", stats.TotalPages).
			Int("unique_forms", stats.UniqueForms).
			Float64("avg_similarity", stats.AverageSimilarity).
			Msg("ğŸ“ˆ åŸŸåç»Ÿè®¡ (Domain statistics)")
	}
	o.domainStatsMutex.RUnlock()
}

// autoAdjustThresholds è‡ªåŠ¨è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼
func (o *Orchestrator) autoAdjustThresholds(ticker <-chan time.Time) {
	for {
		select {
		case <-ticker:
			o.domainStatsMutex.RLock()
			for domain, stats := range o.domainStats {
				if time.Since(stats.LastAdjustment) < time.Minute*10 {
					continue
				}

				// æ ¹æ®å¹³å‡ç›¸ä¼¼åº¦è°ƒæ•´é˜ˆå€¼
				if stats.AverageSimilarity > 0.9 {
					// é¡µé¢ç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œé™ä½é˜ˆå€¼ä»¥å‡å°‘é‡å¤çˆ¬å–
					o.similarityConfig.DOMThreshold = 0.90
					o.similarityConfig.ContentThreshold = 0.85
				} else if stats.AverageSimilarity < 0.5 {
					// é¡µé¢å·®å¼‚è¾ƒå¤§ï¼Œæé«˜é˜ˆå€¼ä»¥çˆ¬å–æ›´å¤šé¡µé¢
					o.similarityConfig.DOMThreshold = 0.75
					o.similarityConfig.ContentThreshold = 0.70
				}

				stats.LastAdjustment = time.Now()
				log.Debug().
					Str("domain", domain).
					Float64("dom_threshold", o.similarityConfig.DOMThreshold).
					Float64("content_threshold", o.similarityConfig.ContentThreshold).
					Msg("Adjusted similarity thresholds")
			}
			o.domainStatsMutex.RUnlock()
		case <-o.ctx.Done():
			return
		}
	}
}

// handleError ç»Ÿä¸€é”™è¯¯å¤„ç†
func (o *Orchestrator) handleError(err error, url string, operation string) bool {
	if err == nil {
		return false
	}

	log.Error().
		Err(err).
		Str("url", url).
		Str("operation", operation).
		Msg("æ“ä½œå¤±è´¥")

	// æ ¹æ®é”™è¯¯ç±»å‹å†³å®šæ˜¯å¦ç»§ç»­
	return o.isCriticalError(err)
}

// isCriticalError åˆ¤æ–­æ˜¯å¦ä¸ºå…³é”®é”™è¯¯
func (o *Orchestrator) isCriticalError(err error) bool {
	if err == nil {
		return false
	}

	errStr := strings.ToLower(err.Error())
	criticalErrors := []string{
		"context canceled",
		"context deadline exceeded",
		"connection refused",
		"no such host",
		"network unreachable",
	}

	for _, critical := range criticalErrors {
		if strings.Contains(errStr, critical) {
			return true
		}
	}

	return false
}

// handleCrawlTask å¤„ç†çˆ¬å–ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ·±åº¦æ£€æŸ¥ã€ç›¸ä¼¼åº¦åˆ†æã€é“¾æ¥å’Œè¯·æ±‚å‘ç°
func (o *Orchestrator) handleCrawlTask(task models.Task, wg *sync.WaitGroup, reporter *output.Reporter, seenURLs *sync.Map, crawlQueue chan models.Task, requestsToScan *[]*models.Request, reqMutex *sync.Mutex) {
	// æ£€æŸ¥ä¸Šä¸‹æ–‡å–æ¶ˆ
	select {
	case <-o.ctx.Done():
		return
	default:
	}

	atomic.AddInt64(&o.stats.urlsProcessed, 1)

	// 0. èŒƒå›´æ£€æŸ¥
	if !o.isInScope(task.URL) {
		log.Debug().Str("url", task.URL).Str("reason", "out_of_scope").Msg("â­ï¸ è·³è¿‡çˆ¬å– (Skipping crawl)")
		reporter.LogUnscopedURL(task.URL)
		return
	}

	if task.Depth >= o.config.Spider.MaxDepth {
		log.Debug().Str("url", task.URL).Int("depth", task.Depth).Str("reason", "max_depth_reached").Msg("â­ï¸ è·³è¿‡çˆ¬å– (Skipping crawl)")
		return
	}

	// 1. URLæ¨¡å¼æ£€æŸ¥
	if o.isURLPatternDuplicate(task.URL) {
		log.Debug().Str("url", task.URL).Str("reason", "duplicate_pattern").Msg("â­ï¸ è·³è¿‡çˆ¬å– (Skipping crawl)")
		atomic.AddInt64(&o.stats.similarPagesSkipped, 1)
		return
	}

	// 2. è·å–é¡µé¢å†…å®¹
	log.Debug().Str("url", task.URL).Msg("â¬‡ï¸ æ­£åœ¨è·å–é¡µé¢ (Fetching page)")
	bodyBytes, err := o.fetchURLWithRetry(task.URL)
	if err != nil {
		if o.handleError(err, task.URL, "fetch") {
			return
		}
		log.Error().Err(err).Str("url", task.URL).Msg("âŒ è·å–URLå¤±è´¥ (Failed to fetch URL)")
		return
	}
	log.Debug().Str("url", task.URL).Int("size", len(bodyBytes)).Msg("âœ… é¡µé¢è·å–æˆåŠŸ (Page fetched successfully)")

	// 3. åˆ†æé¡µé¢ç»“æ„
	log.Debug().Str("url", task.URL).Msg("ğŸ”¬ æ­£åœ¨åˆ†æé¡µé¢ç»“æ„ (Analyzing page structure)")
	pageStructure, err := o.analyzePageStructure(task.URL, bodyBytes)
	if err != nil {
		log.Error().Err(err).Str("url", task.URL).Msg("âŒ é¡µé¢ç»“æ„åˆ†æå¤±è´¥ (Failed to analyze page structure)")
		return
	}

	// 4. ç›¸ä¼¼åº¦æ£€æŸ¥
	if o.isSimilarPage(pageStructure) {
		log.Debug().Str("url", task.URL).Str("reason", "similar_page").Msg("â­ï¸ è·³è¿‡çˆ¬å– (Skipping crawl)")
		atomic.AddInt64(&o.stats.similarPagesSkipped, 1)
		return
	}

	// 5. ä¼ ç»Ÿå»é‡æ£€æŸ¥ï¼ˆä½œä¸ºå¤‡ä»½ï¼‰
	isUnique, err := o.deduplicator.IsUnique(task.URL, bytes.NewReader(bodyBytes))
	if err != nil {
		log.Error().Err(err).Str("url", task.URL).Msg("âŒ å»é‡æ£€æŸ¥å¤±è´¥ (Deduplication check failed)")
		return
	}
	if !isUnique {
		log.Debug().Str("url", task.URL).Str("reason", "duplicate_content").Msg("â­ï¸ è·³è¿‡çˆ¬å– (Skipping crawl)")
		reporter.LogDeDuplicateURL(task.URL)
		atomic.AddInt64(&o.stats.duplicatesSkipped, 1)
		return
	}

	// 6. å­˜å‚¨é¡µé¢ç»“æ„
	o.storePageStructure(task.URL, pageStructure)
	o.updateDomainStatistics(task.URL, pageStructure)

	// 7. çˆ¬å–å’Œè§£æé¡µé¢å†…å®¹
	log.Info().Str("url", task.URL).Msg("ğŸ å¼€å§‹çˆ¬å– (Starting crawl)")
	allLinks, allRequests, err := o.crawler.Crawl(o.ctx, task.URL, bodyBytes)
	if err != nil {
		log.Error().Err(err).Str("url", task.URL).Msg("âŒ çˆ¬å–å¤±è´¥ (Crawl failed)")
		return
	}
	log.Info().
		Str("url", task.URL).
		Int("found_links", len(allLinks)).
		Int("found_requests", len(allRequests)).
		Msg("âœ… çˆ¬å–å®Œæˆ (Crawl finished)")

	reporter.LogURL(task.URL)
	// æ›´æ–°å‚æ•°ç»Ÿè®¡
	for _, req := range allRequests {
		if u, err := url.Parse(req.URL); err == nil {
			atomic.AddInt64(&o.stats.paramsFound, int64(len(u.Query())))
		}
		if req.Method == "POST" {
			// è¿™é‡Œå‡è®¾ Body æ˜¯ urlencoded çš„è¡¨å•
			if params, err := url.ParseQuery(req.Body); err == nil {
				atomic.AddInt64(&o.stats.postParamsFound, int64(len(params)))
				atomic.AddInt64(&o.stats.paramsFound, int64(len(params)))
			}
		}
	}

	log.Debug().Str("url", task.URL).Int("found_links", len(allLinks)).Int("found_requests", len(allRequests)).Msg("ğŸ”— å‘ç°æ–°é“¾æ¥å’Œè¯·æ±‚ (Found new links and requests)")

	// 8. è¿‡æ»¤å’ŒéªŒè¯æ–°å‘ç°çš„é“¾æ¥å’Œè¯·æ±‚
	validLinks := o.filterValidLinks(allLinks)
	validRequests := o.filterValidRequests(allRequests)
	log.Debug().Str("url", task.URL).Int("valid_links", len(validLinks)).Int("valid_requests", len(validRequests)).Msg("ğŸ›¡ï¸ è¿‡æ»¤åæœ‰æ•ˆçš„é“¾æ¥å’Œè¯·æ±‚ (Filtered valid links and requests)")

	// 9. ä¼˜å…ˆå¤„ç†ç»“æ„å·®å¼‚è¾ƒå¤§çš„è¡¨å•
	validRequests = o.prioritizeUniqueFormRequests(validRequests)

	// 10. å°†æ–°ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—
	for _, link := range validLinks {
		if _, loaded := seenURLs.LoadOrStore(link, true); !loaded {
			wg.Add(1)
			select {
			case crawlQueue <- models.Task{URL: link, Depth: task.Depth + 1}:
			case <-o.ctx.Done():
				wg.Done()
				return
			}
		}
	}

	reqMutex.Lock()
	*requestsToScan = append(*requestsToScan, validRequests...)
	reqMutex.Unlock()

	log.Debug().
		Int("found_links", len(allLinks)).
		Int("found_requests", len(allRequests)).
		Int("found_params", int(atomic.LoadInt64(&o.stats.paramsFound))).
		Str("url", task.URL).
		Msg("ğŸ•·ï¸ çˆ¬å–å®Œæˆ (Crawl finished)")
}

// storePageStructure å­˜å‚¨é¡µé¢ç»“æ„ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
func (o *Orchestrator) storePageStructure(url string, structure *PageStructure) {
	timestamped := &TimestampedPageStructure{
		PageStructure: structure,
		Timestamp:     time.Now(),
	}
	o.pageStructures.Store(url, timestamped)
}

// analyzePageStructure åˆ†æé¡µé¢ç»“æ„
func (o *Orchestrator) analyzePageStructure(pageURL string, bodyBytes []byte) (*PageStructure, error) {
	doc, err := html.Parse(bytes.NewReader(bodyBytes))
	if err != nil {
		return nil, fmt.Errorf("è§£æHTMLå¤±è´¥: %w", err)
	}

	structure := &PageStructure{
		FormFields: make(map[string]string),
	}

	// è®¡ç®—DOMç»“æ„å“ˆå¸Œ
	structure.DOMHash = o.calculateDOMHash(doc)

	// è®¡ç®—æ–‡æœ¬å†…å®¹å“ˆå¸Œ
	textContent := o.extractTextContent(doc)
	hash := md5.Sum([]byte(textContent))
	structure.TextHash = fmt.Sprintf("%x", hash)

	// æå–é¡µé¢æ ‡é¢˜
	structure.Title = o.extractTitle(doc)

	// åˆ†æé¡µé¢å…ƒç´ 
	o.analyzeNode(doc, structure)

	return structure, nil
}

// calculateDOMHash è®¡ç®—DOMç»“æ„å“ˆå¸Œï¼ˆä¼˜åŒ–ç‰ˆï¼‰
func (o *Orchestrator) calculateDOMHash(node *html.Node) string {
	builder := builderPool.Get().(*strings.Builder)
	defer func() {
		builder.Reset()
		builderPool.Put(builder)
	}()

	o.traverseDOM(node, builder, 0)

	hash := md5.Sum([]byte(builder.String()))
	return fmt.Sprintf("%x", hash)
}

// traverseDOM éå†DOMç»“æ„
func (o *Orchestrator) traverseDOM(node *html.Node, builder *strings.Builder, depth int) {
	if node == nil {
		return
	}

	// åªè®°å½•ç»“æ„æ€§å…ƒç´ ï¼Œå¿½ç•¥æ–‡æœ¬å†…å®¹å’Œå±æ€§å€¼
	if node.Type == html.ElementNode {
		// æ·»åŠ ç¼©è¿›è¡¨ç¤ºå±‚çº§
		for i := 0; i < depth; i++ {
			builder.WriteString("  ")
		}
		builder.WriteString(node.Data)

		// è®°å½•é‡è¦å±æ€§çš„å­˜åœ¨æ€§ï¼ˆä¸è®°å½•å…·ä½“å€¼ï¼‰
		importantAttrs := []string{"id", "class", "name", "type", "method", "action"}
		for _, attr := range node.Attr {
			for _, important := range importantAttrs {
				if attr.Key == important {
					builder.WriteString(fmt.Sprintf("[%s]", attr.Key))
					break
				}
			}
		}
		builder.WriteString("\n")
	}

	// é€’å½’å¤„ç†å­èŠ‚ç‚¹
	for child := node.FirstChild; child != nil; child = child.NextSibling {
		o.traverseDOM(child, builder, depth+1)
	}
}

// extractTextContent æå–æ–‡æœ¬å†…å®¹
func (o *Orchestrator) extractTextContent(node *html.Node) string {
	builder := builderPool.Get().(*strings.Builder)
	defer func() {
		builder.Reset()
		builderPool.Put(builder)
	}()

	o.extractTextFromNode(node, builder)
	return strings.TrimSpace(builder.String())
}

// extractTextFromNode ä»èŠ‚ç‚¹æå–æ–‡æœ¬
func (o *Orchestrator) extractTextFromNode(node *html.Node, builder *strings.Builder) {
	if node == nil {
		return
	}

	if node.Type == html.TextNode {
		text := strings.TrimSpace(node.Data)
		if text != "" {
			builder.WriteString(text)
			builder.WriteString(" ")
		}
	}

	for child := node.FirstChild; child != nil; child = child.NextSibling {
		o.extractTextFromNode(child, builder)
	}
}

// extractTitle æå–é¡µé¢æ ‡é¢˜
func (o *Orchestrator) extractTitle(node *html.Node) string {
	if node == nil {
		return ""
	}

	if node.Type == html.ElementNode && node.Data == "title" {
		if node.FirstChild != nil && node.FirstChild.Type == html.TextNode {
			return strings.TrimSpace(node.FirstChild.Data)
		}
	}

	for child := node.FirstChild; child != nil; child = child.NextSibling {
		if title := o.extractTitle(child); title != "" {
			return title
		}
	}

	return ""
}

// analyzeNode åˆ†æèŠ‚ç‚¹ï¼Œç»Ÿè®¡å„ç§å…ƒç´ 
func (o *Orchestrator) analyzeNode(node *html.Node, structure *PageStructure) {
	if node == nil {
		return
	}

	if node.Type == html.ElementNode {
		switch node.Data {
		case "input", "textarea", "select":
			structure.InputCount++
			// æå–è¡¨å•å­—æ®µä¿¡æ¯
			name := o.getAttrValue(node, "name")
			fieldType := o.getAttrValue(node, "type")
			if name != "" {
				structure.FormFields[name] = fieldType
			}
		case "a":
			structure.LinkCount++
		case "script":
			structure.ScriptCount++
		}
	}

	// é€’å½’å¤„ç†å­èŠ‚ç‚¹
	for child := node.FirstChild; child != nil; child = child.NextSibling {
		o.analyzeNode(child, structure)
	}
}

// getAttrValue è·å–å±æ€§å€¼
func (o *Orchestrator) getAttrValue(node *html.Node, attrName string) string {
	for _, attr := range node.Attr {
		if attr.Key == attrName {
			return attr.Val
		}
	}
	return ""
}

// isSimilarPage æ£€æŸ¥é¡µé¢ç›¸ä¼¼åº¦
func (o *Orchestrator) isSimilarPage(newStructure *PageStructure) bool {
	var maxSimilarity float64

	o.pageStructures.Range(func(key, value interface{}) bool {
		var existingStructure *PageStructure

		// å¤„ç†æ–°çš„æ—¶é—´æˆ³ç»“æ„
		if timestamped, ok := value.(*TimestampedPageStructure); ok {
			existingStructure = timestamped.PageStructure
		} else if structure, ok := value.(*PageStructure); ok {
			existingStructure = structure
		} else {
			return true // ç»§ç»­éå†
		}

		// è®¡ç®—DOMç›¸ä¼¼åº¦
		domSimilarity := o.calculateDOMSimilarity(newStructure.DOMHash, existingStructure.DOMHash)

		// è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦
		contentSimilarity := o.calculateContentSimilarity(newStructure.TextHash, existingStructure.TextHash)

		// è®¡ç®—è¡¨å•ç›¸ä¼¼åº¦
		formSimilarity := o.calculateFormSimilarity(newStructure.FormFields, existingStructure.FormFields)

		// ç»¼åˆç›¸ä¼¼åº¦
		overallSimilarity := (domSimilarity + contentSimilarity + formSimilarity) / 3.0

		if overallSimilarity > maxSimilarity {
			maxSimilarity = overallSimilarity
		}

		return true // ç»§ç»­éå†
	})

	// æ£€æŸ¥æ˜¯å¦è¶…è¿‡é˜ˆå€¼
	return maxSimilarity > o.similarityConfig.DOMThreshold
}

// calculatePageSimilarity è®¡ç®—é¡µé¢ç›¸ä¼¼åº¦
func (o *Orchestrator) calculatePageSimilarity(structure *PageStructure) float64 {
	if structure == nil {
		return 0.0
	}

	var totalSimilarity float64
	var count int

	o.pageStructures.Range(func(key, value interface{}) bool {
		var existingStructure *PageStructure

		if timestamped, ok := value.(*TimestampedPageStructure); ok {
			existingStructure = timestamped.PageStructure
		} else if structure, ok := value.(*PageStructure); ok {
			existingStructure = structure
		} else {
			return true
		}

		// è®¡ç®—DOMç›¸ä¼¼åº¦
		domSimilarity := o.calculateDOMSimilarity(structure.DOMHash, existingStructure.DOMHash)

		// è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦
		contentSimilarity := o.calculateContentSimilarity(structure.TextHash, existingStructure.TextHash)

		// è®¡ç®—è¡¨å•ç›¸ä¼¼åº¦
		formSimilarity := o.calculateFormSimilarity(structure.FormFields, existingStructure.FormFields)

		// ç»¼åˆç›¸ä¼¼åº¦
		overallSimilarity := (domSimilarity + contentSimilarity + formSimilarity) / 3.0

		totalSimilarity += overallSimilarity
		count++

		return true
	})

	if count == 0 {
		return 0.0
	}

	return totalSimilarity / float64(count)
}

// calculateDOMSimilarity è®¡ç®—DOMç›¸ä¼¼åº¦
func (o *Orchestrator) calculateDOMSimilarity(hash1, hash2 string) float64 {
	if hash1 == hash2 {
		return 1.0
	}
	return 0.0 // ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•
}

// calculateContentSimilarity è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦
func (o *Orchestrator) calculateContentSimilarity(hash1, hash2 string) float64 {
	if hash1 == hash2 {
		return 1.0
	}
	return 0.0 // ç®€åŒ–ç‰ˆæœ¬
}

// calculateFormSimilarity è®¡ç®—è¡¨å•ç›¸ä¼¼åº¦
func (o *Orchestrator) calculateFormSimilarity(form1, form2 map[string]string) float64 {
	if len(form1) == 0 && len(form2) == 0 {
		return 1.0
	}

	if len(form1) == 0 || len(form2) == 0 {
		return 0.0
	}

	// è®¡ç®—å­—æ®µåçš„äº¤é›†
	common := 0
	total := len(form1)
	if len(form2) > total {
		total = len(form2)
	}

	for field := range form1 {
		if _, exists := form2[field]; exists {
			common++
		}
	}

	return float64(common) / float64(total)
}

// isURLPatternDuplicate æ£€æŸ¥URLæ¨¡å¼æ˜¯å¦é‡å¤
func (o *Orchestrator) isURLPatternDuplicate(targetURL string) bool {
	pattern := o.extractURLPattern(targetURL)
	if pattern == "" {
		return false
	}

	_, exists := o.urlPatterns.LoadOrStore(pattern, true)
	return exists
}

// extractURLPattern æå–URLæ¨¡å¼
func (o *Orchestrator) extractURLPattern(targetURL string) string {
	parsedURL, err := url.Parse(targetURL)
	if err != nil {
		return ""
	}

	// ç§»é™¤æŸ¥è¯¢å‚æ•°çš„å€¼ï¼Œåªä¿ç•™å‚æ•°å
	if parsedURL.RawQuery != "" {
		values, err := url.ParseQuery(parsedURL.RawQuery)
		if err != nil {
			return ""
		}

		var paramNames []string
		for name := range values {
			paramNames = append(paramNames, name)
		}
		sort.Strings(paramNames)

		// æ„å»ºæ¨¡å¼ï¼špath + æ’åºåçš„å‚æ•°å
		pattern := parsedURL.Path
		if len(paramNames) > 0 {
			pattern += "?" + strings.Join(paramNames, "&")
		}
		return pattern
	}

	return parsedURL.Path
}

// updateDomainStatistics æ›´æ–°åŸŸåç»Ÿè®¡
func (o *Orchestrator) updateDomainStatistics(pageURL string, structure *PageStructure) {
	parsedURL, err := url.Parse(pageURL)
	if err != nil {
		return
	}

	domain := parsedURL.Host

	o.domainStatsMutex.Lock()
	defer o.domainStatsMutex.Unlock()

	stats, exists := o.domainStats[domain]
	if !exists {
		stats = &DomainStatistics{
			LastAdjustment: time.Now(),
		}
		o.domainStats[domain] = stats
	}

	stats.TotalPages++
	stats.UniqueForms += len(structure.FormFields)

	// ä¿®æ­£å¹³å‡ç›¸ä¼¼åº¦è®¡ç®—
	if stats.TotalPages == 1 {
		stats.AverageSimilarity = 0.5 // åˆå§‹å€¼
	} else {
		// è®¡ç®—å½“å‰é¡µé¢çš„ç›¸ä¼¼åº¦
		currentSimilarity := o.calculatePageSimilarity(structure)
		stats.AverageSimilarity = (stats.AverageSimilarity*float64(stats.TotalPages-1) + currentSimilarity) / float64(stats.TotalPages)
	}
}

// fetchURLWithRetry å¸¦é‡è¯•çš„URLè·å–
func (o *Orchestrator) fetchURLWithRetry(targetURL string) ([]byte, error) {
	var lastErr error

	for attempt := 0; attempt <= o.retryConfig.maxRetries; attempt++ {
		if attempt > 0 {
			log.Debug().Str("url", targetURL).Int("attempt", attempt).Msg("Retrying URL fetch")
			select {
			case <-time.After(o.retryConfig.retryDelay):
			case <-o.ctx.Done():
				return nil, o.ctx.Err()
			}
		}

		resp, err := o.httpClient.Get(o.ctx, targetURL, nil)
		if err != nil {
			lastErr = err
			if !o.isRetryableError(err) {
				break
			}
			continue
		}

		// ä½¿ç”¨é—­åŒ…ç¡®ä¿èµ„æºæ­£ç¡®é‡Šæ”¾
		bodyBytes, readErr := func() ([]byte, error) {
			defer resp.Body.Close()
			return io.ReadAll(resp.Body)
		}()

		if readErr != nil {
			lastErr = readErr
			continue
		}

		return bodyBytes, nil
	}

	return nil, fmt.Errorf("failed after %d attempts: %w", o.retryConfig.maxRetries+1, lastErr)
}

// isRetryableError åˆ¤æ–­é”™è¯¯æ˜¯å¦å¯é‡è¯•
func (o *Orchestrator) isRetryableError(err error) bool {
	if err == nil {
		return false
	}

	errStr := strings.ToLower(err.Error())
	retryableErrors := []string{
		"timeout",
		"connection reset",
		"temporary failure",
		"network is unreachable",
		"connection refused",
	}

	for _, retryable := range retryableErrors {
		if strings.Contains(errStr, retryable) {
			return true
		}
	}

	// æ£€æŸ¥HTTPçŠ¶æ€ç 
	if strings.Contains(errStr, "500") || strings.Contains(errStr, "502") ||
		strings.Contains(errStr, "503") || strings.Contains(errStr, "504") {
		return true
	}

	return false
}

// filterValidLinks è¿‡æ»¤æœ‰æ•ˆé“¾æ¥
func (o *Orchestrator) filterValidLinks(links []string) []string {
	var validLinks []string
	seenLinks := make(map[string]bool)

	for _, link := range links {
		// å»é‡
		if seenLinks[link] {
			continue
		}
		seenLinks[link] = true

		// èŒƒå›´æ£€æŸ¥
		if !o.isInScope(link) {
			continue
		}

		// URLæ ¼å¼æ£€æŸ¥
		if _, err := url.Parse(link); err != nil {
			continue
		}

		validLinks = append(validLinks, link)
	}

	return validLinks
}

// filterValidRequests è¿‡æ»¤æœ‰æ•ˆè¯·æ±‚
func (o *Orchestrator) filterValidRequests(requests []*models.Request) []*models.Request {
	var validRequests []*models.Request
	seenRequests := make(map[string]bool)

	for _, req := range requests {
		// ç”Ÿæˆè¯·æ±‚å”¯ä¸€æ ‡è¯†
		requestKey := fmt.Sprintf("%s:%s:%s", req.Method, req.URL, req.Body)
		
		// å»é‡
		if seenRequests[requestKey] {
			continue
		}
		seenRequests[requestKey] = true

		// èŒƒå›´æ£€æŸ¥
		if !o.isInScope(req.URL) {
			continue
		}

		// è¯·æ±‚å»é‡æ£€æŸ¥
		if o.isRequestDuplicate(req) {
			continue
		}

		validRequests = append(validRequests, req)
	}

	return validRequests
}

// isRequestDuplicate æ£€æŸ¥è¯·æ±‚æ˜¯å¦é‡å¤
func (o *Orchestrator) isRequestDuplicate(req *models.Request) bool {
	// ç”Ÿæˆè¯·æ±‚æŒ‡çº¹
	fingerprint := o.generateRequestFingerprint(req)
	
	_, exists := o.requestDedup.LoadOrStore(fingerprint, true)
	return exists
}

// generateRequestFingerprint ç”Ÿæˆè¯·æ±‚æŒ‡çº¹
func (o *Orchestrator) generateRequestFingerprint(req *models.Request) string {
	parsedURL, err := url.Parse(req.URL)
	if err != nil {
		return req.URL
	}

	// æå–å‚æ•°åï¼ˆå¿½ç•¥å‚æ•°å€¼ï¼‰
	var paramNames []string
	if parsedURL.RawQuery != "" {
		values, _ := url.ParseQuery(parsedURL.RawQuery)
		for name := range values {
			paramNames = append(paramNames, name)
		}
	}

	// å¤„ç†POSTå‚æ•°
	if req.Method == "POST" && req.Body != "" {
		if postValues, err := url.ParseQuery(req.Body); err == nil {
			for name := range postValues {
				paramNames = append(paramNames, "POST:"+name)
			}
		}
	}

	sort.Strings(paramNames)

	// æ„å»ºæŒ‡çº¹ï¼šæ–¹æ³• + è·¯å¾„ + å‚æ•°å
	fingerprint := fmt.Sprintf("%s:%s:%s", 
		req.Method, 
		parsedURL.Path, 
		strings.Join(paramNames, ","))

	return fingerprint
}

// prioritizeUniqueFormRequests ä¼˜å…ˆå¤„ç†ç‹¬ç‰¹çš„è¡¨å•è¯·æ±‚
func (o *Orchestrator) prioritizeUniqueFormRequests(requests []*models.Request) []*models.Request {
	// æŒ‰è¡¨å•ç»“æ„åˆ†ç»„
	formGroups := make(map[string][]*models.Request)
	
	for _, req := range requests {
		formHash := o.calculateFormHash(req)
		formGroups[formHash] = append(formGroups[formHash], req)
	}

	var prioritizedRequests []*models.Request

	// æ¯ä¸ªè¡¨å•ç»“æ„åªå–ä¸€ä¸ªä»£è¡¨æ€§è¯·æ±‚
	for _, group := range formGroups {
		if len(group) > 0 {
			// é€‰æ‹©å‚æ•°æœ€å¤šçš„è¯·æ±‚ä½œä¸ºä»£è¡¨
			representative := group[0]
			maxParams := o.countRequestParams(representative)

			for _, req := range group[1:] {
				paramCount := o.countRequestParams(req)
				if paramCount > maxParams {
					representative = req
					maxParams = paramCount
				}
			}

			prioritizedRequests = append(prioritizedRequests, representative)
		}
	}

	return prioritizedRequests
}

// calculateFormHash è®¡ç®—è¡¨å•å“ˆå¸Œ
func (o *Orchestrator) calculateFormHash(req *models.Request) string {
	var paramNames []string

	// å¤„ç†URLå‚æ•°
	if parsedURL, err := url.Parse(req.URL); err == nil && parsedURL.RawQuery != "" {
		if values, err := url.ParseQuery(parsedURL.RawQuery); err == nil {
			for name := range values {
				paramNames = append(paramNames, "GET:"+name)
			}
		}
	}

	// å¤„ç†POSTå‚æ•°
	if req.Method == "POST" && req.Body != "" {
		if values, err := url.ParseQuery(req.Body); err == nil {
			for name := range values {
				paramNames = append(paramNames, "POST:"+name)
			}
		}
	}

	sort.Strings(paramNames)
	combined := strings.Join(paramNames, ",")
	
	hash := md5.Sum([]byte(combined))
	return fmt.Sprintf("%x", hash)
}

// countRequestParams è®¡ç®—è¯·æ±‚å‚æ•°æ•°é‡
func (o *Orchestrator) countRequestParams(req *models.Request) int {
	count := 0

	// è®¡ç®—URLå‚æ•°
	if parsedURL, err := url.Parse(req.URL); err == nil && parsedURL.RawQuery != "" {
		if values, err := url.ParseQuery(parsedURL.RawQuery); err == nil {
			count += len(values)
		}
	}

	// è®¡ç®—POSTå‚æ•°
	if req.Method == "POST" && req.Body != "" {
		if values, err := url.ParseQuery(req.Body); err == nil {
			count += len(values)
		}
	}

	return count
}

// Close æ¸…ç†èµ„æº
func (o *Orchestrator) Close() error {
	// å–æ¶ˆä¸Šä¸‹æ–‡
	if o.cancel != nil {
		o.cancel()
	}

	// åœæ­¢æ¸…ç†ä»»åŠ¡
	o.stopCleanupTask()

	// å…³é—­æ‰«æå¼•æ“
	if o.scanEngine != nil {
		o.scanEngine.Close()
	}

	return nil
}
